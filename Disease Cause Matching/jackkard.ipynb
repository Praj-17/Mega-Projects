{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cause Matching "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Study Title REV with Sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles = data['Public Title of Study']\n",
    "Causes = data['List of Causes'].dropna()\n",
    "data = Titles + Causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Titles)\n",
    "len(Causes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we have  255 causes and 330 titles to search on.\n",
    "Now, we will filter the titles one by one to get to know the keywords of a title and thier properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to preprocess the Titles we will remove the mostl commonly words (stopwords) out of it so that it doesnt affect out models accuracy. Also, we will remove the underscores, puctuations, special characters, escape sequence characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(sentence):\n",
    "    \"\"\"\n",
    "    It will take the word and covert it to its meanigfull format\n",
    "    finally  final finale finalized ---> Final\n",
    "    \"\"\"\n",
    "    sentence =  \" \".join([lemmatizer.lemmatize(word)for word in sentence.split()])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "def preprocessing(sentence):\n",
    "   \n",
    "   ######################### Preprocessing for new_data ##########################e\n",
    "    #Converting to smaller case\n",
    "    sentence = sentence.lower()\n",
    "    #Stop Words Removal\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    #line break removal\n",
    "    sentence = re.sub(r\"\\r?\\\\n\",\" \", sentence)\n",
    "    #remove special characters\n",
    "    sentence = re.sub(r'\\W+', ' ', sentence)\n",
    "    #remove numbers\n",
    "    sentence = sentence.replace('\\d+', '')\n",
    "    sentence = re.sub(r'\\b\\d+\\b', ' ', sentence)\n",
    "    #remove punctuation\n",
    "    sentence = sentence.replace('[^\\w\\s]','')\n",
    "    #remove underscore\n",
    "    sentence = sentence.replace('_', ' ')\n",
    "    #stemming documents(removing ing, ly, s)\n",
    "    #remove stop words and finally stem\n",
    "    sentence = lemmatize(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(\" #Prajwal is a good boy.\\n. his fav number is 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the desired processes on the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles = [preprocessing(i) for i in Titles if type(i) is str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same preprocessing on CAuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Causes = [preprocessing(i) for i in Causes if type(i) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Causes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Approaching with jackkard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic\n",
    " - A title with similar words as that in the cause must be more related to the cause. Hence, since the word count in intersection matters we will be going for Jackkard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach - \n",
    "- Take a title and a cause. compare the words that are closer to (similar to)the words in the cause. and count the Jackkard similariy between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_similars(word):\n",
    "    similars = []\n",
    "    similars_processed = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            similars.append( preprocessing(i.name()))\n",
    "    \n",
    "    return similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similars(\"smart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_from_string(sentence:str)->str:\n",
    "    similars = ''\n",
    "    for i in sentence.split(' '):\n",
    "        similars += ' '.join(get_similars(i))\n",
    "    return preprocessing(similars)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_from_string(\"smart boy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate Jackkard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Cause and Y:Title\n",
    "def jaccard_similarity(x,y):\n",
    "  \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "  cause_similars = get_similar_from_string(x)\n",
    "  intersection_cardinality = len(set.intersection(*[set(cause_similars), set(y)]))\n",
    "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "  return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(\"bollywood\", \"Tv actor shit timpass villain story\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Datframe to store the desired objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df = pd.DataFrame(columns= ['Titles', 'Buffer'])\n",
    "for title in Titles:\n",
    "    # Df['Titles'].append(title)\n",
    "    for id, cause in enumerate(Causes):\n",
    "        # Df[f'Cause {id+1}'] = Df['Buffer']\n",
    "        # Df[f'Cause {id+1}'].append(cause)\n",
    "        similarity = 100*jaccard_similarity(cause, title)\n",
    "        \n",
    "        if similarity > 100:\n",
    "            print(similarity)\n",
    "            print(\"Titles\", title)\n",
    "            print(\"Cause\", cause)\n",
    "        # Df[f'Similarity {id+1}'].append(cause)\n",
    "        \n",
    "      \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "941096ab4a304f3509822dba92ee1d36e30c4bf17f0e34fdfd12300d4a2ccfca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
